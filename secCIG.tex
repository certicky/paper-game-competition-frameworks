\vskip 20mm

\section{CIG: Computational\\ Intelligence in Games}\label{subsecCIG}

The CIG StarCraft AI Competition has been a part of the program of the IEEE Computational Intelligence in Games (CIG) conference since August 2011.  Since the date of the CIG competition was usually just before the AIIDE competition, many of the bots submitted to both competitions ended up being nearly identical, therefore the CIG competition has several rule differences with AIIDE in order to keep the results interesting. The biggest rule difference was that the CIG competition did not disclose which maps would be used for the competition, meaning that the bots could not use any hard-coded map information like they could in the AIIDE competition.

\subsection{CIG Competition History}
The CIG conference is well known for hosting and organizing many AI related competitions, such as the Mario AI Competition and the General Video Game Playing Competition, and in 2010 the first CIG StarCraft AI Competition was held. Organized by Johan Hagelback, Mike Preuss, and Ben Weber, the CIG 2010 competition was to have a single game mode similar to the tech-limited Tournament 3 from the AIIDE 2010 competition, but using the Terran race instead of the Protoss race. Unfortunately the first year of the CIG competition met with several serious technical problems, which stemmed from the decision to use custom made StarCraft maps as opposed to traditional human-tested maps, which caused the Brood War Terrain Analysis (BWTA) software to crash for many of the bots. Because of these frequent crashes, it was decided that no winner could be announced for the competition. Mike Preuss and his team members then successfully organized the CIG competition each year from 2011 to 2013.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{fig/cig-starcraft-runs.png}
  \caption{In the computing lab, the Tournament Manager software distributes the game matches to multiple machines.}
  \label{figCIGruns}
\end{figure}

Since 2014, the Sejong University team (led by Kyung-Joong Kim) has organized the CIG StarCraft AI competition at the IEEE CIG conference. In order to provide a more diverse competition, CIG rules and settings have changed from year to year, and are shown in Table~\ref{tableTournaments}. 

Throughout its history, CIG has had multiple changes in the selection of tournament management software, open-source policy and map pool announcement policy. The tournament management software (see section X) is used to distribute the matches over multiple machines on the network and to automate the competition operation (see the photo in Figure~\ref{figCIGruns}). Although CIG organizers developed their own JAVA-based TM sofware, the AIIDE TM has been used for the competition since 2014~(see details in Section \ref{secTournamentManagerSoftware}). Since 2016, CIG has enforced an open source code policy, and all of the bots' source code are published after the competition. Unlike the AIIDE competition, the CIG map pool is not known to the participants before the competition to promote generalization ability of the entries. However, it was found out that participants usually did not exploit the map as a prior knowledge when they prepared the entries. Therefore, the set of competition's maps could be announced in advance. 
 
In the 2016 competition, the organizers introduced a two-stage evaluation to the competition, where only half of the entries advance to the second stage based on the win ratio of the first stage. The concept of the qualification stage is not new in competitions, for example, the simulated car racing competition adopted the two-stage competitions divided into qualification stage and main race \cite{loiacono20102009}. Since the single-pool round robin format is based on win percentage only, it is important to get high average win ratio against all the opponents. The CIG organizers introduced two pools with the intent to reduce the chance that the top ranking bots win just by exploiting lower ranked bots to boost their win ratio. Bots were randomly split into two groups, and then the top half of bots from each group were brought together for a final stage group, to be played as round robin, with all learned information being deleted before the beginning of the final stage. 

\subsection{2017 CIG Competition}\label{subsecCIGnews}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{fig/cig-results.png}
  \caption{Detailed results of the CIG 2017 competition final stage.}
  \label{figCIGresults}
\end{figure}

In the 2017 CIG competition, the two stage format was changed back to the single group format. The reason was that the participants did not seem to change their strategy to consider the two-stage tournament, and it did not seem to have much of an effect on the final results. In the future, CIG organizers consider adopting the SWISS-system widely used in the board game community. In this setting, a player does not play against all the other opponents. Instead, the participants are paired with the opponents with similar scores. Such system usually produces the final outcome similar to round-robin with a smaller number of rounds. 

In 2017, CIG organizers tried to play as many games as possible, and reached 125 rounds with 190 games per round, which resulted in 23750 games in the two-round format. Currently, AI bots often use multiple pre-prepared strategies and adapt them or their selection against specific opponents. The more games played during a tournament, the more experience allows them to learn which strategies are good against which opponents. As in the 2017 AIIDE competition, many bots implemented learning strategies which dramatically increased their win rates over time. Detailed results of the top 6 bots in the competition can be seen in figure \ref{figCIGresults}.

After the 2017 CIG competition Sejong University organized a special event where human players were matched against the AI bots. The human players included one novice player (ladder rating around 1100), one middle-level player (around 1500), and a professional gamer: Byung-Gu Song. AI bots in the event were ZZZKBot, the winner of CIG 2017 competition, tscmoo bot, 2nd rank in CIG 2017, and MJBOT, an AI bot specially designed against human players. The MJBOT has been developed since June 2017 by Cognition Intelligence Laboratory to beat novice/middle-level human players. Each human player played a single game against each AI bot (9 games). The novice human player lost two games against ZZZKBOT and TSCMOO, but won the game against MJBOT, which was not able to finish the game due to the programming bug. In the next session, the middle-level human player lost all three games against the AI bots. Finally, the professional human player Byung-Gu Song won against all the AI bots\footnote{\url{http://cilab.sejong.ac.kr/}}. This suggests that the AI bots have a potential to compete against novice and middle-level players, but are not yet at the level of professional gamers. 

%\begin{figure}[t]
%  \centering
%  \includegraphics[width=0.5\textwidth]{fig/song_human_ai.png}
%  \caption{Professional player Byung-Gu Song playing against AI bots.}
%  \label{figureSong}
%\end{figure} 
